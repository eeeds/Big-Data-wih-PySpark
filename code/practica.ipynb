{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8689f3f1-428c-4641-9c4f-257eb71a9324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "##Master − It is the URL of the cluster it connects to.\n",
    "##appName − Name of your job.\n",
    "sc = SparkContext(\"local\", \"First App\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e9ef1c-3a92-42b3-8efa-3ae4987560bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The version of Spark Context in the PySpark shell is 3.3.0\n",
      "The Python version of Spark Context in the PySpark shell is 3.10\n",
      "The master of Spark Context in the PySpark shell is local\n"
     ]
    }
   ],
   "source": [
    "# Print the version of SparkContext\n",
    "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\n",
    "\n",
    "# Print the Python version of SparkContext\n",
    "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n",
    "\n",
    "# Print the master of SparkContext\n",
    "print(\"The master of Spark Context in the PySpark shell is\", sc.master)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4aadaa-6e42-4f7f-bb80-f0d54a7c65ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Interactive Use of PySpark\n",
    "Spark comes with an interactive Python shell in which PySpark is already installed in it. PySpark shell is useful for basic testing and debugging and it is quite powerful. The easiest way to demonstrate the power of PySpark’s shell is to start using it. In this example, you'll load a simple list containing numbers ranging from 1 to 100 in the PySpark shell.\n",
    "\n",
    "The most important thing to understand here is that we are not creating any SparkContext object because PySpark automatically creates the SparkContext object named sc, by default in the PySpark shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "702ca792-3397-4751-bfa1-706f9178aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Python list named numb containing the numbers 1 to 100.\n",
    "numb = range(1, 100)\n",
    "#Load the list into Spark using Spark Context's parallelize method and assign it to a variable spark_data.\n",
    "spark_data = sc.parallelize(numb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d35a0-237c-4f27-a9ca-444d25fab4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
